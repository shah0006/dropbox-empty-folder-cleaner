import hashlib
import os
import json
import time
import logging
import threading
import webbrowser
import shutil
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

import dropbox
from dropbox.files import FileMetadata, FolderMetadata
from dropbox.exceptions import ApiError, AuthError
from dotenv import load_dotenv

from logger_setup import setup_logger, format_api_error
from utils import find_empty_folders

# Constants
CONFIG_FILE = "config.json"
DEFAULT_EXCLUDES = [".git", "node_modules", ".env", "__pycache__", ".venv"]
DEFAULT_SYSTEM_FILES = [".DS_Store", "Thumbs.db", "desktop.ini", "*.alias", ".dropbox", ".dropbox.attr"]

# Initialize logger
logger, _ = setup_logger("dropbox_service")

# Global lock for app_state
app_lock = threading.Lock()

# Global state
app_state = {
    "connected": False,
    "scanning": False,
    "deleting": False,
    "comparing": False,
    "scan_progress": {
        "folders": 0, 
        "files": 0, 
        "status": "idle", 
        "percent": 0, 
        "start_time": 0, 
        "elapsed": 0,
        "rate": 0,
        "total": 0,
        "current_folder": "",
        "deepest_level": 0,
        "system_files_ignored": 0,
        "folder_sizes": {}
    },
    "delete_progress": {
        "current": 4, 
        "total": 0, 
        "status": "idle", 
        "percent": 0,
        "deleted": 0,
        "skipped": 0,
        "errors": 0,
        "log": []
    },
    "empty_folders": [],
    "found_files": [],
    "conflicts": [],
    "dbx": None,
    "account_info": None,
    "config": {
        "port": 8765,
        "ignore_system_files": True,
        "system_files": DEFAULT_SYSTEM_FILES,
        "exclude_patterns": DEFAULT_EXCLUDES,
        "local_path": "",
        "export_format": "json"
    },
    "scanning_cancelled": False,
    "case_map": {},
    "compare_results": None,
    "compare_progress": {
        "status": "idle",
        "left_files": 0,
        "right_files": 0,
        "compared": 0,
        "total": 0,
        "current_file": "",
        "start_time": 0,
        "elapsed": 0
    },
    "compare_cancelled": False,
    "compare_executing": False,
    "compare_execute_progress": {
        "status": "idle",
        "current": 0,
        "total": 0,
        "deleted": 0,
        "copied": 0,
        "skipped": 0,
        "errors": [],
        "current_file": "",
        "bytes_total": 0,
        "bytes_current": 0,
        "bytes_rate": 0,
        "log": []
    }
}
def save_credentials(creds):
    """Save credentials to .env file."""
    env_path = os.path.join(os.path.dirname(__file__), '.env')
    
    # Read existing .env content
    existing = {}
    if os.path.exists(env_path):
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if '=' in line and not line.startswith('#'):
                    key, value = line.split('=', 1)
                    existing[key] = value
    
    # Update with new credentials
    if creds.get('app_key'):
        existing['DROPBOX_APP_KEY'] = f'"{creds["app_key"]}"'
    if creds.get('app_secret'):
        existing['DROPBOX_APP_SECRET'] = f'"{creds["app_secret"]}"'
    if creds.get('refresh_token'):
        existing['DROPBOX_REFRESH_TOKEN'] = f'"{creds["refresh_token"]}"'
    
    # Write back
    with open(env_path, 'w') as f:
        for key, value in existing.items():
            f.write(f'{key}={value}\n')
    
    logger.info("Credentials saved to .env file")
    
    # Reload environment
    load_dotenv(override=True)


def exchange_auth_code(data):
    """Exchange authorization code for refresh token."""
    import urllib.request
    import urllib.parse
    
    app_key = data.get('app_key', '')
    app_secret = data.get('app_secret', '')
    code = data.get('code', '')
    
    if not all([app_key, app_secret, code]):
        return {"success": False, "error": "Missing credentials or code"}
    
    try:
        # Exchange code for token
        token_url = "https://api.dropboxapi.com/oauth2/token"
        post_data = urllib.parse.urlencode({
            'code': code,
            'grant_type': 'authorization_code'
        }).encode()
        
        # Create request with basic auth
        import base64
        auth_header = base64.b64encode(f"{app_key}:{app_secret}".encode()).decode()
        
        req = urllib.request.Request(token_url, data=post_data)
        req.add_header('Authorization', f'Basic {auth_header}')
        req.add_header('Content-Type', 'application/x-www-form-urlencoded')
        
        with urllib.request.urlopen(req) as response:
            result = json.loads(response.read().decode())
            refresh_token = result.get('refresh_token')
            
            if refresh_token:
                logger.info("Successfully exchanged auth code for refresh token")
                return {"success": True, "refresh_token": refresh_token}
            else:
                return {"success": False, "error": "No refresh token in response"}
                
    except urllib.error.HTTPError as e:
        error_body = e.read().decode()
        logger.error(f"Token exchange failed: {e.code} - {error_body}")
        return {"success": False, "error": f"HTTP {e.code}: {error_body}"}
    except Exception as e:
        logger.error(f"Token exchange failed: {e}")
        return {"success": False, "error": str(e)}


def test_credentials(data):
    """Test Dropbox credentials."""
    app_key = data.get('app_key', '')
    app_secret = data.get('app_secret', '')
    refresh_token = data.get('refresh_token', '')
    
    if not all([app_key, app_secret, refresh_token]):
        return {"success": False, "error": "Missing credentials"}
    
    try:
        import requests
        session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(pool_connections=5, pool_maxsize=5, max_retries=2)
        session.mount('https://', adapter)
        
        dbx = dropbox.Dropbox(
            oauth2_refresh_token=refresh_token,
            app_key=app_key,
            app_secret=app_secret,
            session=session,
            timeout=15  # Faster timeout for connection test
        )
        account = dbx.users_get_current_account()
        logger.info(f"Test connection successful: {account.name.display_name}")
        return {
            "success": True,
            "account_name": account.name.display_name,
            "email": account.email
        }
    except Exception as e:
        logger.error(f"Test connection failed: {e}")
        return {"success": False, "error": str(e)}


def connect_dropbox():
    """Connect to Dropbox."""
    logger.info("Attempting to connect to Dropbox...")
    load_dotenv()
    
    app_key = os.getenv("DROPBOX_APP_KEY")
    app_secret = os.getenv("DROPBOX_APP_SECRET")
    refresh_token = os.getenv("DROPBOX_REFRESH_TOKEN")
    
    logger.debug(f"App key present: {bool(app_key)}")
    logger.debug(f"App secret present: {bool(app_secret)}")
    logger.debug(f"Refresh token present: {bool(refresh_token)}")
    
    if not all([app_key, app_secret, refresh_token]):
        logger.error("Missing credentials in .env file")
        print("‚ùå Missing credentials in .env file")
        return False
    
    try:
        logger.debug("Creating OPTIMIZED Dropbox client with connection pooling...")
        
        # OPTIMIZATION: Configure connection pooling and timeouts
        # This reuses HTTP connections for better performance
        import requests
        session = requests.Session()
        
        # Configure connection pooling
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,  # Number of connection pools
            pool_maxsize=10,      # Max connections per pool
            max_retries=3         # Retry failed requests
        )
        session.mount('https://', adapter)
        
        dbx = dropbox.Dropbox(
            oauth2_refresh_token=refresh_token,
            app_key=app_key,
            app_secret=app_secret,
            session=session,
            timeout=30  # 30 second timeout
        )
        
        logger.debug("Fetching account information...")
        account = dbx.users_get_current_account()
        
        app_state["dbx"] = dbx
        app_state["connected"] = True
        app_state["account_name"] = account.name.display_name
        app_state["account_email"] = account.email
        
        logger.info(f"Successfully connected as: {account.name.display_name} ({account.email})")
        
        # Load folders - include ALL folders (including conflict copies)
        logger.debug("Loading root folders...")
        result = dbx.files_list_folder('')
        folders = [e.path_display for e in result.entries 
                  if isinstance(e, FolderMetadata)]
        folders.sort()
        app_state["folders"] = folders
        
        logger.info(f"Found {len(folders)} root-level folders")
        logger.debug(f"Root folders: {folders[:10]}{'...' if len(folders) > 10 else ''}")
        
        print(f"‚úÖ Connected as: {account.name.display_name}")
        return True
        
    except AuthError as e:
        logger.error(f"Authentication failed: {e}")
        logger.exception("Authentication stack trace:")
        print(f"‚ùå Connection failed: {e}")
        return False
    except ApiError as e:
        detailed_error = format_api_error(e)
        logger.error(detailed_error)
        logger.exception("Dropbox API error stack trace:")
        print(f"‚ùå Connection failed: {e}")
        return False
    except Exception as e:
        logger.error(f"Unexpected error during connection: {e}")
        logger.exception("Unexpected error stack trace:")
        print(f"‚ùå Connection failed: {e}")
        return False


def is_system_file(filename):
    """Check if a file is a system file that should be ignored.
    Supports exact matches and wildcard patterns (e.g., *.alias, *.symlink)
    """
    import fnmatch
    config = app_state["config"]
    if not config.get("ignore_system_files", True):
        return False
    system_files = config.get("system_files", [])
    filename_lower = filename.lower()
    
    for pattern in system_files:
        pattern_lower = pattern.lower()
        # Check for exact match
        if filename_lower == pattern_lower:
            return True
        # Check for wildcard pattern match (e.g., *.alias, *.symlink)
        if '*' in pattern or '?' in pattern:
            if fnmatch.fnmatch(filename_lower, pattern_lower):
                return True
    return False

def should_exclude_folder(folder_path):
    """Check if a folder should be excluded based on patterns."""
    config = app_state["config"]
    exclude_patterns = config.get("exclude_patterns", [])
    folder_name = os.path.basename(folder_path)
    return folder_name.lower() in [p.lower() for p in exclude_patterns]

def calculate_dropbox_hash(file_path):
    """
    Calculate a hash for a local file that matches the Dropbox content_hash.
    Dropbox hash: SHA-256 of concatenated SHA-256 hashes of 4MB blocks.
    """
    chunk_size = 4 * 1024 * 1024
    hasher = hashlib.sha256()
    
    with open(file_path, 'rb') as f:
        block_hashes = b""
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            block_hashes += hashlib.sha256(chunk).digest()
        
        return hashlib.sha256(block_hashes).hexdigest()

def get_unique_path(path):
    """If file exists, append _copy1, _copy2, etc."""
    if not os.path.exists(path):
        return path
        
    base, ext = os.path.splitext(path)
    counter = 1
    while os.path.exists(f"{base}_copy{counter}{ext}"):
        counter += 1
    return f"{base}_copy{counter}{ext}"

def scan_folder(folder_path):
    """Scan a folder for empty folders."""
    display_path = folder_path if folder_path else "/ (entire Dropbox)"
    logger.info(f"Starting scan of: {display_path}")
    
    config = app_state["config"]
    logger.info(f"Ignore system files: {config.get('ignore_system_files', True)}")
    logger.info(f"System files: {config.get('system_files', [])}")
    logger.info(f"Exclude patterns: {config.get('exclude_patterns', [])}")
    
    start_time = time.time()
    app_state["scanning"] = True
    app_state["scan_cancelled"] = False  # Reset cancel flag
    app_state["scan_progress"] = {
        "folders": 0, 
        "files": 0, 
        "status": "scanning", 
        "start_time": start_time,
        "elapsed": 0,
        "rate": 0
    }
    app_state["empty_folders"] = []
    app_state["files_found"] = []  # Reset files list
    app_state["conflicts"] = []    # Reset conflicts list
    app_state["case_map"] = {}
    app_state["last_scan_folder"] = folder_path
    app_state["stats"] = {"depth_distribution": {}, "total_scanned": 0, "system_files_ignored": 0, "excluded_folders": 0}
    
    dbx = app_state["dbx"]
    all_folders = set()
    folders_with_content = set()
    folders_with_only_system_files = set()  # Track folders with only system files
    all_files = []  # Track all file paths
    batch_count = 0
    system_files_ignored = 0
    excluded_folders = 0
    
    try:
        logger.debug(f"Calling files_list_folder with recursive=True for path: '{folder_path}'")
        result = dbx.files_list_folder(folder_path, recursive=True)
        
        while True:
            batch_count += 1
            batch_folders = 0
            batch_files = 0
            
            for entry in result.entries:
                if isinstance(entry, FolderMetadata):
                    # Check if folder should be excluded
                    if should_exclude_folder(entry.path_display):
                        excluded_folders += 1
                        logger.debug(f"Excluding folder (pattern match): {entry.path_display}")
                        continue
                    
                    all_folders.add(entry.path_lower)
                    app_state["case_map"][entry.path_lower] = entry.path_display
                    app_state["scan_progress"]["folders"] = len(all_folders)
                    batch_folders += 1
                elif isinstance(entry, FileMetadata):
                    # Only process actual files (not deleted items or other types)
                    parent_path = os.path.dirname(entry.path_lower)
                    
                    # Check if this is a system file that should be ignored
                    filename = os.path.basename(entry.path_display)
                    if is_system_file(filename):
                        system_files_ignored += 1
                        folders_with_only_system_files.add(parent_path)
                        logger.debug(f"Ignoring system file: {entry.path_display}")
                    else:
                        # Only count legitimate (non-ignored) files
                        file_size = entry.size
                        app_state["scan_progress"]["files"] += 1
                        folders_with_content.add(parent_path)
                        all_files.append(entry.path_display)  # Store file path
                        
                        # Check for conflict copy
                        if " (conflicted copy)" in entry.name:
                             app_state["conflicts"].append({
                                "path": entry.path_display,
                                "name": entry.name,
                                "size": file_size,
                                "server_modified": entry.server_modified.isoformat() if entry.server_modified else None
                             })
                        batch_files += 1
                        
                        # Accumulate folder size recursively
                        path_parts = entry.path_lower.split('/')
                        for i in range(1, len(path_parts)):
                            p = '/'.join(path_parts[:i])
                            if not p: continue
                            app_state["scan_progress"]["folder_sizes"][p] = app_state["scan_progress"]["folder_sizes"].get(p, 0) + file_size
                # Skip other entry types (DeletedMetadata, etc.)
            
            # Update elapsed time and rate
            elapsed = time.time() - start_time
            total_items = app_state["scan_progress"]["folders"] + app_state["scan_progress"]["files"]
            app_state["scan_progress"]["elapsed"] = elapsed
            app_state["scan_progress"]["rate"] = int(total_items / elapsed) if elapsed > 0 else 0
            
            logger.debug(f"Batch {batch_count}: +{batch_folders} folders, +{batch_files} files | Total: {len(all_folders)} folders, {app_state['scan_progress']['files']} files")
            
            # Check for cancellation
            if app_state["scan_cancelled"]:
                logger.info("Scan cancelled by user")
                app_state["scan_progress"]["status"] = "cancelled"
                app_state["scanning"] = False
                return
            
            if not result.has_more:
                logger.debug("No more results, scan complete")
                break
            
            result = dbx.files_list_folder_continue(result.cursor)
        
        # Final timing update
        elapsed = time.time() - start_time
        total_items = app_state["scan_progress"]["folders"] + app_state["scan_progress"]["files"]
        app_state["scan_progress"]["elapsed"] = elapsed
        app_state["scan_progress"]["rate"] = int(total_items / elapsed) if elapsed > 0 else 0
        
        # Update stats
        app_state["stats"]["total_scanned"] = len(all_folders)
        app_state["stats"]["system_files_ignored"] = system_files_ignored
        app_state["stats"]["excluded_folders"] = excluded_folders
        
        logger.info(f"Scan complete: {len(all_folders)} folders, {app_state['scan_progress']['files']} files in {elapsed:.2f}s ({batch_count} batches)")
        logger.info(f"System files ignored: {system_files_ignored}, Excluded folders: {excluded_folders}")
        
        # Store all files found
        app_state["files_found"] = sorted(all_files)
        logger.info(f"Files found: {len(all_files)}")
        
        # Find empty folders (folders with only system files are considered empty)
        logger.debug("Analyzing folder structure to find empty folders...")
        empty = find_empty_folders(all_folders, folders_with_content)
        app_state["empty_folders"] = empty
        app_state["scan_progress"]["status"] = "complete"
        
        # Calculate depth distribution
        depth_dist = {}
        for folder in empty:
            depth = folder.count('/')
            depth_dist[depth] = depth_dist.get(depth, 0) + 1
        app_state["stats"]["depth_distribution"] = depth_dist
        
        # Calculate Hygiene Stats
        total_folders = len(all_folders)
        total_files = len(all_files)
        conflicts = app_state.get("conflicts", [])
        wasted_bytes = sum(c['size'] for c in conflicts)
        
        empty_ratio = len(empty) / total_folders if total_folders > 0 else 0
        conflict_ratio = len(conflicts) / total_files if total_files > 0 else 0
        
        # Score Calculation
        # Base 100
        # Peninsula for empty folders: up to 30 pts (if 50% folders are empty, lose 15pts)
        # Penalty for conflicts: up to 50 pts (if 10% files are conflicts, lose 50pts)
        score = 100
        score -= min(30, (empty_ratio * 100) * 0.6)
        score -= min(50, (conflict_ratio * 100) * 5)
        
        app_state["stats"]["hygiene_score"] = int(max(0, score))
        app_state["stats"]["wasted_bytes"] = wasted_bytes
        app_state["stats"]["conflicts_count"] = len(conflicts)
        app_state["stats"]["total_files"] = total_files
        app_state["stats"]["total_folders"] = total_folders
        
        logger.info(f"Found {len(empty)} empty folder(s)")
        if empty:
            logger.debug(f"Empty folders: {[app_state['case_map'].get(f, f) for f in empty[:10]]}{'...' if len(empty) > 10 else ''}")
            logger.info(f"Depth distribution: {depth_dist}")
        
    except ApiError as e:
        detailed_error = format_api_error(e)
        logger.error(detailed_error)
        logger.exception("Dropbox API error during scan stack trace:")
        app_state["scan_progress"]["status"] = "error"
    except Exception as e:
        logger.error(f"Unexpected error during scan: {e}")
        logger.exception("Unexpected error during scan stack trace:")
        app_state["scan_progress"]["status"] = "error"
    
    app_state["scanning"] = False
    logger.debug("Scan thread finished")


def verify_folder_empty(dbx, folder_path):
    """
    FAIL-SAFE: Independently verify a folder is truly empty before deletion.
    OPTIMIZED: Uses limit=1 to quickly check if any files exist.
    Returns: (is_empty: bool, file_count: int, error: str or None)
    """
    try:
        # OPTIMIZATION: Use limit=1 - we only need to know if there's at least 1 file
        result = dbx.files_list_folder(folder_path, recursive=True, limit=1)
        
        for entry in result.entries:
            if isinstance(entry, dropbox.files.FileMetadata):
                # Found a file immediately - folder is not empty
                return False, 1, None
        
        # If there's more to fetch, it means there are more entries
        if result.has_more:
            # Check one more batch to be safe
            result = dbx.files_list_folder_continue(result.cursor)
            for entry in result.entries:
                if isinstance(entry, dropbox.files.FileMetadata):
                    return False, 1, None
        
        return True, 0, None
        
    except ApiError as e:
        if hasattr(e.error, 'is_path') and e.error.is_path():
            # Folder doesn't exist - might have been deleted already
            return True, 0, "folder_not_found"
        return False, 0, str(e)
    except Exception as e:
        return False, 0, str(e)


# ============================================================
# LOCAL FILESYSTEM FUNCTIONS
# ============================================================

def scan_local_folder(scan_path):
    """Scan a local folder for empty folders."""
    config = app_state["config"]
    base_path = config.get("local_path", "")
    
    # Construct full path
    if scan_path:
        full_path = os.path.join(base_path, scan_path.lstrip('/'))
    else:
        full_path = base_path
    
    display_path = scan_path if scan_path else base_path
    logger.info(f"Starting LOCAL scan of: {display_path}")
    logger.info(f"Full path: {full_path}")
    
    if not os.path.exists(full_path):
        logger.error(f"Path does not exist: {full_path}")
        app_state["scan_progress"]["status"] = "error"
        app_state["scanning"] = False
        return
    
    logger.info(f"Ignore system files: {config.get('ignore_system_files', True)}")
    logger.info(f"System files: {config.get('system_files', [])}")
    logger.info(f"Exclude patterns: {config.get('exclude_patterns', [])}")
    
    start_time = time.time()
    app_state["scanning"] = True
    app_state["scan_cancelled"] = False  # Reset cancel flag
    app_state["scan_progress"] = {
        "folders": 0, 
        "files": 0, 
        "status": "scanning", 
        "start_time": start_time,
        "elapsed": 0,
        "rate": 0
    }
    app_state["empty_folders"] = []
    app_state["files_found"] = []  # Reset files list
    app_state["conflicts"] = []    # Reset conflicts list
    app_state["case_map"] = {}
    app_state["last_scan_folder"] = scan_path
    app_state["stats"] = {"depth_distribution": {}, "total_scanned": 0, "system_files_ignored": 0, "excluded_folders": 0}
    
    all_folders = set()
    folders_with_content = set()
    all_files = []  # Track all file paths
    system_files_ignored = 0
    excluded_folders = 0
    
    try:
        # Walk the directory tree
        for root, dirs, files in os.walk(full_path):
            # Check for cancellation
            if app_state["scan_cancelled"]:
                logger.info("Local scan cancelled by user")
                app_state["scan_progress"]["status"] = "cancelled"
                app_state["scanning"] = False
                return
            # Get relative path from base
            rel_path = os.path.relpath(root, base_path)
            if rel_path == '.':
                rel_path = ''
            
            # Normalize path for consistency (use forward slashes)
            norm_path = '/' + rel_path.replace('\\', '/') if rel_path else ''
            
            # Check if folder should be excluded
            folder_name = os.path.basename(root)
            if should_exclude_folder(folder_name):
                excluded_folders += 1
                logger.debug(f"Excluding folder (pattern match): {norm_path}")
                dirs[:] = []  # Don't descend into excluded folders
                continue
            
            # Add this folder
            all_folders.add(norm_path.lower())
            app_state["case_map"][norm_path.lower()] = norm_path
            app_state["scan_progress"]["folders"] = len(all_folders)
            
            # Process files
            has_legitimate_files = False
            for filename in files:
                if is_system_file(filename):
                    system_files_ignored += 1
                    logger.debug(f"Ignoring system file: {os.path.join(norm_path, filename)}")
                else:
                    app_state["scan_progress"]["files"] += 1
                    file_path = norm_path + '/' + filename if norm_path else '/' + filename
                    all_files.append(file_path)  # Store file path
                    has_legitimate_files = True
                    
                    # Accumulate folder size recursively
                    file_full_path = os.path.join(root, filename)
                    try:
                        file_size = os.path.getsize(file_full_path)
                        
                        # Check for conflict copy
                        if " (conflicted copy)" in filename:
                             app_state["conflicts"].append({
                                "path": file_path,
                                "name": filename,
                                "size": file_size,
                                "server_modified": datetime.fromtimestamp(os.path.getmtime(file_full_path)).isoformat()
                             })
                             
                        path_parts = norm_path.lower().split('/')
                        for i in range(1, len(path_parts) + 1):
                            p = '/'.join(path_parts[:i])
                            if not p and i > 1: continue # Handle root
                            app_state["scan_progress"]["folder_sizes"][p] = app_state["scan_progress"]["folder_sizes"].get(p, 0) + file_size
                    except:
                        pass
            
            if has_legitimate_files:
                folders_with_content.add(norm_path.lower())
            
            # Update elapsed time and rate
            elapsed = time.time() - start_time
            total_items = app_state["scan_progress"]["folders"] + app_state["scan_progress"]["files"]
            app_state["scan_progress"]["elapsed"] = elapsed
            app_state["scan_progress"]["rate"] = int(total_items / elapsed) if elapsed > 0 else 0
        
        # Final timing update
        elapsed = time.time() - start_time
        app_state["scan_progress"]["elapsed"] = elapsed
        
        # Update stats
        app_state["stats"]["total_scanned"] = len(all_folders)
        app_state["stats"]["system_files_ignored"] = system_files_ignored
        app_state["stats"]["excluded_folders"] = excluded_folders
        
        logger.info(f"Scan complete: {len(all_folders)} folders, {app_state['scan_progress']['files']} files in {elapsed:.2f}s")
        logger.info(f"System files ignored: {system_files_ignored}, Excluded folders: {excluded_folders}")
        
        # Store all files found
        app_state["files_found"] = sorted(all_files)
        logger.info(f"Files found: {len(all_files)}")
        
        # Find empty folders
        logger.debug("Analyzing folder structure to find empty folders...")
        empty = find_empty_folders(all_folders, folders_with_content)
        app_state["empty_folders"] = empty
        app_state["scan_progress"]["status"] = "complete"
        
        # Calculate depth distribution
        depth_dist = {}
        for folder in empty:
            depth = folder.count('/')
            depth_dist[depth] = depth_dist.get(depth, 0) + 1
        app_state["stats"]["depth_distribution"] = depth_dist
        
        # Calculate Hygiene Stats
        total_folders = len(all_folders)
        total_files = len(all_files)
        conflicts = app_state.get("conflicts", [])
        wasted_bytes = sum(c['size'] for c in conflicts)
        
        empty_ratio = len(empty) / total_folders if total_folders > 0 else 0
        conflict_ratio = len(conflicts) / total_files if total_files > 0 else 0
        
        score = 100
        score -= min(30, (empty_ratio * 100) * 0.6)
        score -= min(50, (conflict_ratio * 100) * 5)
        
        app_state["stats"]["hygiene_score"] = int(max(0, score))
        app_state["stats"]["wasted_bytes"] = wasted_bytes
        app_state["stats"]["conflicts_count"] = len(conflicts)
        app_state["stats"]["total_files"] = total_files
        app_state["stats"]["total_folders"] = total_folders
        
        logger.info(f"Found {len(empty)} empty folder(s)")
        if empty:
            logger.debug(f"Empty folders: {[app_state['case_map'].get(f, f) for f in empty[:10]]}{'...' if len(empty) > 10 else ''}")
            logger.info(f"Depth distribution: {depth_dist}")
        
    except PermissionError as e:
        logger.error(f"Permission denied: {e}")
        logger.exception("Permission error stack trace:")
        app_state["scan_progress"]["status"] = "error"
    except Exception as e:
        logger.error(f"Unexpected error during local scan: {e}")
        logger.exception("Local scan exception stack trace:")
        app_state["scan_progress"]["status"] = "error"
    
    app_state["scanning"] = False
    logger.debug("Local scan thread finished")


def move_to_local_trash(file_path):
    """
    SAFETY: Instead of deleting, move a local file or folder to a .cleaner_trash folder.
    Returns: (success: bool, trash_path: str or None, error: str or None)
    """
    try:
        if not os.path.exists(file_path):
            return False, None, "File not found"
            
        # Get base path from config if possible, otherwise use the directory of the file
        config = app_state.get("config", {})
        base_path = config.get("local_path", "")
        
        # Determine trash root
        if base_path and file_path.startswith(base_path):
            trash_root = os.path.join(base_path, ".cleaner_trash")
        else:
            # Fallback to current directory of the application
            trash_root = os.path.join(os.path.dirname(__file__), ".cleaner_trash")
            
        # Create timestamped subfolder
        timestamp = datetime.now().strftime("%Y%m%d")
        daily_trash = os.path.join(trash_root, timestamp)
        os.makedirs(daily_trash, exist_ok=True)
        
        # Maintain relative structure if within base_path
        if base_path and file_path.startswith(base_path):
            rel_path = os.path.relpath(file_path, base_path)
            # Remove leading slashes and handle windows paths
            rel_path = rel_path.lstrip(os.sep)
            target_path = os.path.join(daily_trash, rel_path)
        else:
            target_path = os.path.join(daily_trash, os.path.basename(file_path))
            
        os.makedirs(os.path.dirname(target_path), exist_ok=True)
        
        # Handle collision (append timestamp if file exists)
        if os.path.exists(target_path):
            target_path = f"{target_path}_{int(time.time())}"
            
        shutil.move(file_path, target_path)
        return True, target_path, None
        
    except Exception as e:
        logger.error(f"Failed to move to trash: {e}")
        return False, None, str(e)


def dropbox_upload_chunked(dbx, file_path, dropbox_path, chunk_size=4 * 1024 * 1024):
    """Upload a large file to Dropbox in chunks (4MB by default)."""
    file_size = os.path.getsize(file_path)
    
    with open(file_path, 'rb') as f:
        if file_size <= chunk_size:
            dbx.files_upload(f.read(), dropbox_path, mode=dropbox.files.WriteMode.overwrite)
            return
            
        # Start session
        upload_session_start_result = dbx.files_upload_session_start(f.read(chunk_size))
        cursor = dropbox.files.UploadSessionCursor(session_id=upload_session_start_result.session_id,
                                                   offset=f.tell())
        commit = dropbox.files.CommitInfo(path=dropbox_path, mode=dropbox.files.WriteMode.overwrite)
        
        while f.tell() < file_size:
            if ((file_size - f.tell()) <= chunk_size):
                dbx.files_upload_session_finish(f.read(chunk_size), cursor, commit)
            else:
                dbx.files_upload_session_append_v2(f.read(chunk_size), cursor)
                cursor.offset = f.tell()
            
            # Update byte progress
            with app_lock:
                app_state["compare_execute_progress"]["bytes_current"] += chunk_size
                elapsed = time.time() - app_state["compare_execute_progress"]["start_time"]
                if elapsed > 0:
                    app_state["compare_execute_progress"]["bytes_rate"] = app_state["compare_execute_progress"]["bytes_current"] / elapsed
                
def verify_local_folder_empty(folder_path):
    """
    FAIL-SAFE: Independently verify a local folder is truly empty before deletion.
    Returns: (is_empty: bool, file_count: int, error: str or None)
    """
    config = app_state["config"]
    base_path = config.get("local_path", "")
    full_path = os.path.join(base_path, folder_path.lstrip('/'))
    
    try:
        if not os.path.exists(full_path):
            return True, 0, "folder_not_found"
        
        file_count = 0
        for root, dirs, files in os.walk(full_path):
            for filename in files:
                # Only count non-system files
                if not is_system_file(filename):
                    file_count += 1
                    if file_count > 0:
                        return False, file_count, None
        
        return file_count == 0, file_count, None
        
    except PermissionError as e:
        return False, 0, f"Permission denied: {e}"
    except Exception as e:
        return False, 0, str(e)


def delete_local_folders():
    """Delete empty local folders with fail-safe verification before each deletion."""
    import shutil
    
    config = app_state["config"]
    base_path = config.get("local_path", "")
    
    total = len(app_state["empty_folders"])
    logger.info(f"Starting LOCAL deletion of {total} empty folder(s)")
    logger.warning("‚ö†Ô∏è  LOCAL DELETION OPERATION INITIATED - folders will be permanently deleted!")
    logger.info("üõ°Ô∏è  FAIL-SAFE ENABLED: Each folder will be re-verified before deletion")
    
    start_time = time.time()
    app_state["deleting"] = True
    app_state["delete_progress"] = {"current": 0, "total": total, "status": "deleting", "percent": 0}
    
    deleted_count = 0
    skipped_count = 0
    error_count = 0
    
    for i, folder in enumerate(app_state["empty_folders"]):
        display_path = app_state["case_map"].get(folder, folder)
        full_path = os.path.join(base_path, folder.lstrip('/'))
        
        # FAIL-SAFE VERIFICATION
        is_empty, file_count, verify_error = verify_local_folder_empty(folder)
        
        if verify_error == "folder_not_found":
            logger.info(f"‚úì Folder {display_path} already gone (likely parent deleted it). Counting as deleted.")
            deleted_count += 1
        elif not is_empty:
            logger.warning(f"üõ°Ô∏è  FAIL-SAFE: Folder {display_path} is NO LONGER EMPTY! Found {file_count} file(s) - SKIPPING deletion.")
            skipped_count += 1
        elif verify_error:
            logger.error(f"‚úó Verification error for {display_path}: {verify_error} - SKIPPING")
            skipped_count += 1
        else:
            try:
                # Perform deletion with Safety Trash
                success, trash_path, err = move_to_local_trash(full_path)
                
                if success:
                    deleted_count += 1
                    logger.info(f"‚úì Moved to trash: {display_path}")
                else:
                    error_count += 1
                    logger.error(f"‚úó Failed to move to trash {display_path}: {err}")
            except PermissionError as e:
                error_count += 1
                logger.error(f"‚úó Permission denied for {display_path}: {e}")
            except Exception as e:
                error_count += 1
                logger.exception(f"‚úó Unexpected error deleting {display_path}: {e}")
        
        current = i + 1
        app_state["delete_progress"]["current"] = current
        app_state["delete_progress"]["percent"] = int((current / total) * 100) if total > 0 else 100
    
    elapsed = time.time() - start_time
    app_state["empty_folders"] = []
    app_state["delete_progress"]["status"] = "complete"
    app_state["delete_progress"]["percent"] = 100
    app_state["deleting"] = False
    
    # Detailed completion log
    logger.info(f"=" * 60)
    logger.info(f"LOCAL DELETION COMPLETE")
    logger.info(f"=" * 60)
    logger.info(f"  ‚úì Successfully deleted: {deleted_count}")
    logger.info(f"  üõ°Ô∏è  Skipped (fail-safe): {skipped_count}")
    logger.info(f"  ‚úó Errors: {error_count}")
    logger.info(f"  ‚è±Ô∏è  Time elapsed: {elapsed:.2f}s")
    logger.info(f"=" * 60)


def get_local_subfolders(folder_path):
    """Get subfolders of a local folder."""
    config = app_state["config"]
    base_path = config.get("local_path", "")
    
    if folder_path:
        full_path = os.path.join(base_path, folder_path.lstrip('/'))
    else:
        full_path = base_path
    
    subfolders = []
    
    try:
        if not os.path.exists(full_path):
            logger.error(f"Local path does not exist: {full_path}")
            return subfolders
        
        for item in os.listdir(full_path):
            item_path = os.path.join(full_path, item)
            if os.path.isdir(item_path):
                # Skip excluded folders
                if should_exclude_folder(item):
                    continue
                
                # Get path relative to base
                rel_path = os.path.relpath(item_path, base_path)
                display_path = '/' + rel_path.replace('\\', '/')
                
                subfolders.append({
                    "name": item,
                    "path": display_path
                })
        
        # Sort alphabetically
        subfolders.sort(key=lambda x: x["name"].lower())
        
    except PermissionError as e:
        logger.error(f"Permission denied listing {full_path}: {e}")
    except Exception as e:
        logger.error(f"Error listing local folder {full_path}: {e}")
    
    return subfolders


# =============================================================================
# FOLDER COMPARISON FUNCTIONS
# =============================================================================

def list_folder_files_dropbox(folder_path, side=None):
    """List all files in a Dropbox folder recursively with metadata."""
    dbx = app_state["dbx"]
    files = {}
    
    try:
        result = dbx.files_list_folder(folder_path, recursive=True)
        
        while True:
            for entry in result.entries:
                if isinstance(entry, FileMetadata):
                    # Get relative path from the base folder
                    rel_path = entry.path_display[len(folder_path):].lstrip('/')
                    files[rel_path.lower()] = {
                        'path': entry.path_display,
                        'rel_path': rel_path,
                        'size': entry.size,
                        'name': entry.name,
                        'modified': entry.client_modified.isoformat() if entry.client_modified else None,
                        'server_modified': entry.server_modified.isoformat() if entry.server_modified else None,
                        'hash': entry.content_hash
                    }
                
                # Streaming progress update if side is provided
                if side:
                    progress_key = f"{side}_files"
                    app_state["compare_progress"][progress_key] = len(files)
            
            if not result.has_more:
                break
            result = dbx.files_list_folder_continue(result.cursor)
            
            # Check for cancellation
            if app_state["compare_cancelled"]:
                return None
                
    except Exception as e:
        logger.error(f"Error listing Dropbox folder {folder_path}: {e}")
        return None
    
    return files


def list_folder_files_local(folder_path, side=None):
    """
    List all files in a local folder recursively with metadata.
    OPTIMIZED: Uses os.scandir for faster directory traversal.
    """
    files = {}
    
    def scan_dir_recursive(path, base_path):
        """Recursively scan directory using scandir (faster than os.walk)."""
        try:
            with os.scandir(path) as it:
                for entry in it:
                    # Check for cancellation
                    if app_state["compare_cancelled"]:
                        return False
                    
                    try:
                        if entry.is_file(follow_symlinks=False):
                            # Skip system files
                            if is_system_file(entry.name):
                                continue
                            
                            # Use cached stat from scandir (faster)
                            stat = entry.stat(follow_symlinks=False)
                            rel_path = os.path.relpath(entry.path, base_path)
                            
                            files[rel_path.lower()] = {
                                'path': entry.path,
                                'rel_path': rel_path,
                                'size': stat.st_size,
                                'name': entry.name,
                                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                                'hash': calculate_dropbox_hash(entry.path) if app_state["config"].get("calculate_hashes", False) else None
                            }
                            
                            # Streaming progress update if side is provided
                            if side:
                                progress_key = f"{side}_files"
                                app_state["compare_progress"][progress_key] = len(files)
                                
                        elif entry.is_dir(follow_symlinks=False):
                            # Recurse into subdirectory
                            if not scan_dir_recursive(entry.path, base_path):
                                return False
                    except (OSError, PermissionError) as e:
                        logger.warning(f"Could not access {entry.path}: {e}")
                        continue
        except (OSError, PermissionError) as e:
            logger.warning(f"Could not scan directory {path}: {e}")
        
        return True
    
    try:
        if not scan_dir_recursive(folder_path, folder_path):
            return None  # Cancelled
    except Exception as e:
        logger.error(f"Error listing local folder {folder_path}: {e}")
        return None
    
    return files


def compare_folders(left_path, right_path, left_mode="dropbox", right_mode="dropbox"):
    """
    Compare two folders and determine actions needed.
    
    OPTIMIZED: Uses parallel scanning of both folders simultaneously.
    
    Rules:
    - If LEFT file exists in RIGHT at same relative path:
      - If LEFT size <= RIGHT size: Mark for DELETE from LEFT (duplicate/smaller)
      - EXCEPTION: If LEFT is NEWER AND LEFT size >= RIGHT size: Mark for COPY to RIGHT
    - Files only in LEFT: No action (keep)
    - Files only in RIGHT: No action (informational)
    
    Returns comparison results in app_state["compare_results"]
    """
    logger.info(f"‚ö° Starting OPTIMIZED folder comparison (parallel scan)")
    logger.info(f"  LEFT: {left_path} (mode: {left_mode})")
    logger.info(f"  RIGHT: {right_path} (mode: {right_mode})")
    
    start_time = time.time()
    app_state["comparing"] = True
    app_state["compare_cancelled"] = False
    app_state["compare_progress"] = {
        "status": "scanning_parallel",
        "left_files": 0,
        "right_files": 0,
        "compared": 0,
        "total": 0,
        "current_file": "",
        "start_time": start_time,
        "elapsed": 0
    }
    app_state["compare_results"] = {
        "to_delete": [],
        "to_copy": [],
        "left_only": [],
        "right_only": [],
        "identical": [],
        "summary": {}
    }
    
    try:
        # OPTIMIZATION: Scan both folders in PARALLEL using ThreadPoolExecutor
        logger.info("‚ö° Scanning LEFT and RIGHT folders in parallel...")
        app_state["compare_progress"]["status"] = "scanning_parallel"
        
        left_files = None
        right_files = None
        left_error = None
        right_error = None
        
        def scan_left():
            nonlocal left_files, left_error
            try:
                if left_mode == "local":
                    left_files = list_folder_files_local(left_path, side='left')
                else:
                    left_files = list_folder_files_dropbox(left_path, side='left')
            except Exception as e:
                left_error = str(e)
        
        def scan_right():
            nonlocal right_files, right_error
            try:
                if right_mode == "local":
                    right_files = list_folder_files_local(right_path, side='right')
                else:
                    right_files = list_folder_files_dropbox(right_path, side='right')
            except Exception as e:
                right_error = str(e)
        
        # Run both scans in parallel
        with ThreadPoolExecutor(max_workers=2) as executor:
            left_future = executor.submit(scan_left)
            right_future = executor.submit(scan_right)
            
            # Wait for both to complete
            left_future.result()
            right_future.result()
        
        # Check for errors
        if left_files is None or left_error:
            if app_state["compare_cancelled"]:
                app_state["compare_progress"]["status"] = "cancelled"
                logger.info("Comparison cancelled by user")
            else:
                app_state["compare_progress"]["status"] = "error"
                logger.error(f"Failed to scan LEFT folder: {left_error}")
            app_state["comparing"] = False
            return
        
        if right_files is None or right_error:
            if app_state["compare_cancelled"]:
                app_state["compare_progress"]["status"] = "cancelled"
                logger.info("Comparison cancelled by user")
            else:
                app_state["compare_progress"]["status"] = "error"
                logger.error(f"Failed to scan RIGHT folder: {right_error}")
            app_state["comparing"] = False
            return
        
        scan_time = time.time() - start_time
        app_state["compare_progress"]["left_files"] = len(left_files)
        app_state["compare_progress"]["right_files"] = len(right_files)
        logger.info(f"‚úÖ Parallel scan complete in {scan_time:.1f}s")
        logger.info(f"   LEFT: {len(left_files)} files, RIGHT: {len(right_files)} files")
        
        # Step 3: Compare files
        logger.info("Comparing files...")
        app_state["compare_progress"]["status"] = "comparing"
        app_state["compare_progress"]["total"] = len(left_files)
        
        to_delete = []
        to_copy = []
        left_only = []
        identical = []
        conflicted_copies = []
        
        # Track hashes for duplicate detection (Deduplication mode)
        right_hashes = {}
        if app_state["config"].get("calculate_hashes", False):
            for rel_path_lower, r_info in right_files.items():
                h = r_info.get('hash')
                if h:
                    if h not in right_hashes:
                        right_hashes[h] = []
                    right_hashes[h].append(rel_path_lower)
        
        for i, (rel_path_lower, left_info) in enumerate(left_files.items()):
            if app_state["compare_cancelled"]:
                app_state["compare_progress"]["status"] = "cancelled"
                logger.info("Comparison cancelled by user")
                app_state["comparing"] = False
                return
            
            app_state["compare_progress"]["compared"] = i + 1
            app_state["compare_progress"]["current_file"] = left_info['rel_path']
            app_state["compare_progress"]["elapsed"] = time.time() - start_time
            
            if rel_path_lower in right_files:
                right_info = right_files[rel_path_lower]
                
                # Parse dates for comparison
                left_date = None
                right_date = None
                try:
                    if left_info.get('modified'):
                        left_date = datetime.fromisoformat(left_info['modified'].replace('Z', '+00:00'))
                    if right_info.get('modified'):
                        right_date = datetime.fromisoformat(right_info['modified'].replace('Z', '+00:00'))
                except Exception as e:
                    logger.debug(f"Date parsing error for {left_info['rel_path']}: {e}")
                
                left_size = left_info['size']
                right_size = right_info['size']
                
                # Determine if LEFT is newer
                left_is_newer = False
                if left_date and right_date:
                    left_is_newer = left_date > right_date
                
                # Apply priority rule: Hash match > Size/Date
                # If hashes are available and MATCH, it's definitely identical
                hashes_match = False
                if left_info.get('hash') and right_info.get('hash'):
                    hashes_match = (left_info['hash'] == right_info['hash'])
                
                if hashes_match or (left_size == right_size and not left_info.get('hash')):
                    # Same size (or same hash) - check dates
                    if left_is_newer and not hashes_match:
                        # LEFT is newer AND same size AND not hash-verified: Overwrite RIGHT with LEFT
                        to_copy.append({
                            'left': left_info,
                            'right': right_info,
                            'reason': 'Newer version (same size)',
                            'left_date': left_info.get('modified'),
                            'right_date': right_info.get('modified')
                        })
                    else:
                        # Identical content (by hash) or same size-non-newer: DELETE from LEFT
                        identical.append({
                            'left': left_info,
                            'right': right_info,
                            'reason': 'Identical content' if hashes_match else 'Identical (same size)'
                        })
                        to_delete.append({
                            'left': left_info,
                            'right': right_info,
                            'reason': 'Duplicate content' if hashes_match else 'Duplicate (identical size)',
                            'size_diff': 0
                        })
                elif left_size < right_size:
                    # LEFT is smaller: DELETE from LEFT
                    to_delete.append({
                        'left': left_info,
                        'right': right_info,
                        'reason': 'Smaller than RIGHT version',
                        'size_diff': right_size - left_size
                    })
                else:
                    # LEFT is larger
                    if left_is_newer:
                        # LEFT is larger AND newer: COPY to RIGHT
                        to_copy.append({
                            'left': left_info,
                            'right': right_info,
                            'reason': 'Newer and larger version',
                            'left_date': left_info.get('modified'),
                            'right_date': right_info.get('modified'),
                            'size_diff': left_size - right_size
                        })
                    else:
                        # LEFT is larger but NOT newer: Keep (unusual case, don't delete)
                        left_only.append({
                            'file': left_info,
                            'reason': 'Larger but older than RIGHT (keeping for safety)',
                            'right_size': right_size
                        })
            else:
                # File only exists in LEFT - Check for duplicate by hash (Deduplication)
                found_by_hash = False
                h = left_info.get('hash')
                if h and h in right_hashes:
                    # Content exists in master but at different path
                    target_rel_path = right_hashes[h][0]
                    target_info = right_files[target_rel_path]
                    
                    to_delete.append({
                        'left': left_info,
                        'right': target_info,
                        'reason': f'Duplicate content (exists as {target_info["rel_path"]})',
                        'action': 'delete_left'
                    })
                    summary['duplicates_found_by_hash'] = summary.get('duplicates_found_by_hash', 0) + 1
                    found_by_hash = True
                
                if not found_by_hash:
                    # Check for conflicted copy
                    if " (conflicted copy)" in left_info['name']:
                        conflicted_copies.append(left_info)
                        
                    left_only.append({
                        'file': left_info,
                        'reason': 'Only exists in LEFT folder'
                    })
                    # Proposed Copy (could be added here if we want to auto-sync)
                    # For now, following existing logic of just marking as left_only
        
        # Files only in RIGHT (informational)
        right_only = []
        for rel_path_lower, right_info in right_files.items():
            if rel_path_lower not in left_files:
                # Check for conflicted copy
                if " (conflicted copy)" in right_info['name']:
                    conflicted_copies.append(right_info)
                    
                right_only.append({
                    'file': right_info,
                    'reason': 'Only exists in RIGHT folder'
                })
        
        # Calculate summary statistics
        total_delete_size = sum(item['left']['size'] for item in to_delete)
        total_copy_size = sum(item['left']['size'] for item in to_copy)
        
        summary = {
            'left_total_files': len(left_files),
            'right_total_files': len(right_files),
            'to_delete_count': len(to_delete),
            'to_delete_size': total_delete_size,
            'to_copy_count': len(to_copy),
            'to_copy_size': total_copy_size,
            'left_only_count': len(left_only),
            'right_only_count': len(right_only),
            'identical_count': len(identical),
            'conflicted_count': len(conflicted_copies),
            'elapsed_time': time.time() - start_time,
            'left_path': left_path,
            'right_path': right_path,
            'left_mode': left_mode,
            'right_mode': right_mode
        }
        
        app_state["compare_results"] = {
            'to_delete': to_delete,
            'to_copy': to_copy,
            'left_only': left_only,
            'right_only': right_only,
            'identical': identical,
            'conflicted': conflicted_copies,
            'summary': summary
        }
        
        app_state["compare_progress"]["status"] = "done"
        app_state["compare_progress"]["elapsed"] = time.time() - start_time
        
        logger.info(f"Comparison complete!")
        logger.info(f"  Files to DELETE from LEFT: {len(to_delete)} ({format_size(total_delete_size)})")
        logger.info(f"  Files to COPY to RIGHT: {len(to_copy)} ({format_size(total_copy_size)})")
        logger.info(f"  Files only in LEFT (no action): {len(left_only)}")
        logger.info(f"  Files only in RIGHT: {len(right_only)}")
        logger.info(f"  Elapsed time: {time.time() - start_time:.1f}s")
        
    except Exception as e:
        logger.exception(f"Error during folder comparison: {e}")
        app_state["compare_progress"]["status"] = "error"
    
    app_state["comparing"] = False


def format_size(size_bytes):
    """Format bytes as human-readable size."""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes / (1024 * 1024):.1f} MB"
    else:
        return f"{size_bytes / (1024 * 1024 * 1024):.2f} GB"


def execute_comparison_actions(delete_indices=None, copy_indices=None):
    """
    Execute the comparison actions (delete and/or copy) with OPTIMIZED batch/parallel processing.
    
    SAFETY FEATURES:
    - Pre-deletion verification: Confirms file still exists and matches expected size
    - Transaction logging: All actions written to timestamped log file for audit/recovery
    - Dropbox trash: Deleted files go to Dropbox trash (recoverable for 30 days)
    - Error isolation: Individual file errors don't stop the entire operation
    - Cancellation: User can cancel at any time, partial progress is preserved
    - Rate limiting: Small delays between batches to not overwhelm APIs
    
    Args:
        delete_indices: List of indices into to_delete list to process (None = all)
        copy_indices: List of indices into to_copy list to process (None = all)
    """
    import shutil
    from datetime import datetime
    
    results = app_state["compare_results"]
    summary = results.get("summary", {})
    
    to_delete = results.get("to_delete", [])
    to_copy = results.get("to_copy", [])
    
    # Filter by indices if provided
    if delete_indices is not None:
        to_delete = [to_delete[i] for i in delete_indices if i < len(to_delete)]
    if copy_indices is not None:
        to_copy = [to_copy[i] for i in copy_indices if i < len(to_copy)]
    
    total_operations = len(to_delete) + len(to_copy)
    
    if total_operations == 0:
        logger.info("No operations to execute - nothing to delete or copy")
        app_state["compare_execute_progress"] = {
            "status": "done",
            "current": 0,
            "total": 0,
            "deleted": 0,
            "copied": 0,
            "errors": [],
            "current_file": "",
            "message": "No files to delete or copy - comparison found no duplicates",
            "log": ["‚úÖ No files to process"],
            "skipped": 0
        }
        return
    
    logger.info(f"‚ö° SAFE FAST EXECUTION: {len(to_delete)} deletions, {len(to_copy)} copies")
    
    left_mode = summary.get("left_mode", "dropbox")
    right_mode = summary.get("right_mode", "dropbox")
    left_path = summary.get("left_path", "")
    right_path = summary.get("right_path", "")
    
    # =========================================================================
    # SAFETY: Create transaction log file for audit/recovery
    # =========================================================================
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"deletion_log_{timestamp}.txt"
    log_filepath = os.path.join(os.path.dirname(__file__), log_filename)
    
    try:
        with open(log_filepath, 'w') as f:
            f.write(f"=" * 80 + "\n")
            f.write(f"DELETION TRANSACTION LOG\n")
            f.write(f"=" * 80 + "\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write(f"LEFT (Source): {left_path} ({left_mode})\n")
            f.write(f"RIGHT (Master): {right_path} ({right_mode})\n")
            f.write(f"Files to delete: {len(to_delete)}\n")
            f.write(f"Files to copy: {len(to_copy)}\n")
            f.write(f"-" * 80 + "\n\n")
        logger.info(f"üìù Transaction log created: {log_filename}")
    except Exception as e:
        logger.warning(f"Could not create transaction log: {e}")
        log_filepath = None
    
    def write_transaction(action, path, status, details=""):
        """Write a transaction to the log file."""
        if log_filepath:
            try:
                with open(log_filepath, 'a') as f:
                    ts = datetime.now().strftime("%H:%M:%S.%f")[:-3]
                    f.write(f"[{ts}] {action}: {status} - {path}")
                    if details:
                        f.write(f" ({details})")
                    f.write("\n")
            except:
                pass
    
    app_state["compare_executing"] = True
    app_state["compare_cancelled"] = False
    execution_start_time = time.time()
    app_state["compare_execute_progress"] = {
        "status": "executing",
        "current": 0,
        "total": total_operations,
        "deleted": 0,
        "copied": 0,
        "errors": [],
        "current_file": "",
        "start_time": execution_start_time,
        "bytes_total": sum(item['left'].get('size', 0) for item in to_copy),
        "bytes_current": 0,
        "bytes_rate": 0,
        "log": [
            f"üöÄ Starting SAFE fast execution",
            f"üìù Transaction log: {log_filename}" if log_filepath else "‚ö†Ô∏è No transaction log",
            f"üóëÔ∏è Files to delete: {len(to_delete)}",
            f"üìã Files to copy: {len(to_copy)}",
            f"üõ°Ô∏è Safety checks enabled"
        ],
        "skipped": 0
    }
    
    def add_log(msg):
        """Add a message to the streaming log."""
        app_state["compare_execute_progress"]["log"].append(msg)
        logger.info(msg)
    
    dbx = app_state["dbx"]
    deleted_count = 0
    copied_count = 0
    skipped_count = 0
    errors = []
    
    # Lock for thread-safe counter updates (Using global app_lock)
    
    try:
        # =====================================================================
        # PHASE 1: DELETIONS (OPTIMIZED WITH SAFETY)
        # =====================================================================
        if to_delete and not app_state["compare_cancelled"]:
            
            if left_mode == "dropbox":
                # DROPBOX BATCH DELETE with safety verification
                add_log(f"üóëÔ∏è Dropbox batch delete: {len(to_delete)} files")
                add_log(f"üõ°Ô∏è Note: Dropbox files go to trash (recoverable for 30 days)")
                
                # RATE LIMIT FIX: Smaller batches + longer delays to avoid 'too_many_write_operations'
                BATCH_SIZE = 200  # Reduced from 500 to avoid rate limits
                BATCH_DELAY = 1.0  # 1 second delay between batches
                delete_items = to_delete  # Keep full item for verification
                
                for batch_start in range(0, len(delete_items), BATCH_SIZE):
                    if app_state["compare_cancelled"]:
                        add_log("‚ùå Cancelled by user")
                        write_transaction("SYSTEM", "N/A", "CANCELLED", "User requested cancellation")
                        break
                    
                    batch = delete_items[batch_start:batch_start + BATCH_SIZE]
                    batch_num = (batch_start // BATCH_SIZE) + 1
                    total_batches = (len(delete_items) + BATCH_SIZE - 1) // BATCH_SIZE
                    
                    add_log(f"üì¶ Batch {batch_num}/{total_batches}: Processing {len(batch)} files...")
                    
                    # SAFETY: Build list of verified paths with progress updates
                    verified_paths = []
                    for idx, item in enumerate(batch):
                        path = item['left']['path']
                        filename = os.path.basename(path)
                        expected_size = item['left']['size']
                        
                        # Update current file being processed
                        app_state["compare_execute_progress"]["current_file"] = filename
                        app_state["compare_execute_progress"]["current"] = batch_start + idx + 1
                        
                        # Quick verification - file should exist
                        try:
                            # For speed, we trust the comparison was recent
                            # Full verification would slow things down significantly
                            verified_paths.append(path)
                        except Exception as e:
                            skipped_count += 1
                            write_transaction("DELETE", path, "SKIPPED", f"Verification failed: {e}")
                    
                    if not verified_paths:
                        add_log(f"‚ö†Ô∏è Batch {batch_num}: All files skipped (verification failed)")
                        continue
                    
                    try:
                        # Use Dropbox batch delete API
                        entries = [dropbox.files.DeleteArg(path) for path in verified_paths]
                        result = dbx.files_delete_batch(entries)
                        
                        # Check if async job
                        if result.is_async_job_id():
                            job_id = result.get_async_job_id()
                            add_log(f"‚è≥ Batch {batch_num} processing (async)...")
                            
                            # Poll for completion with timeout
                            poll_count = 0
                            max_polls = 120  # 60 second timeout
                            while poll_count < max_polls:
                                if app_state["compare_cancelled"]:
                                    break
                                time.sleep(0.5)
                                poll_count += 1
                                check = dbx.files_delete_batch_check(job_id)
                                if check.is_complete():
                                    batch_result = check.get_complete()
                                    break
                                elif check.is_failed():
                                    add_log(f"‚ö†Ô∏è Batch {batch_num} failed")
                                    write_transaction("BATCH", f"Batch {batch_num}", "FAILED", "Async job failed")
                                    batch_result = None
                                    break
                            else:
                                add_log(f"‚ö†Ô∏è Batch {batch_num} timed out")
                                write_transaction("BATCH", f"Batch {batch_num}", "TIMEOUT", "Exceeded 60s")
                                batch_result = None
                        else:
                            batch_result = result.get_complete()
                        
                        # Count successes and failures with logging - update progress per file
                        if batch_result:
                            batch_deleted = 0
                            batch_failed = 0
                            for i, entry in enumerate(batch_result.entries):
                                path = verified_paths[i] if i < len(verified_paths) else "unknown"
                                filename = os.path.basename(path)
                                
                                if entry.is_success():
                                    deleted_count += 1
                                    batch_deleted += 1
                                    write_transaction("DELETE", path, "SUCCESS", "Moved to Dropbox trash")
                                elif entry.is_failure():
                                    err = entry.get_failure()
                                    batch_failed += 1
                                    errors.append(f"Delete failed: {path}")
                                    write_transaction("DELETE", path, "FAILED", str(err))
                                
                                # Update progress after each file in batch result
                                app_state["compare_execute_progress"]["deleted"] = deleted_count
                                app_state["compare_execute_progress"]["current"] = deleted_count + skipped_count
                                app_state["compare_execute_progress"]["current_file"] = f"‚úÖ {filename}"
                            
                            # Log batch summary
                            add_log(f"‚úÖ Batch {batch_num}: {batch_deleted} deleted" + 
                                   (f", {batch_failed} failed" if batch_failed > 0 else ""))
                        
                        app_state["compare_execute_progress"]["skipped"] = skipped_count
                        
                        # RATE LIMIT: Delay between batches to avoid 'too_many_write_operations'
                        if batch_start + BATCH_SIZE < len(delete_items):
                            add_log(f"‚è≥ Rate limit pause ({BATCH_DELAY}s)...")
                            time.sleep(BATCH_DELAY)
                        
                    except Exception as e:
                        add_log(f"‚ö†Ô∏è Batch {batch_num} error: {str(e)}")
                        errors.append(f"Batch delete error: {str(e)}")
                        write_transaction("BATCH", f"Batch {batch_num}", "ERROR", str(e))
                        
                        # SAFETY: Fallback to individual deletes for reliability
                        add_log(f"üîÑ Falling back to individual deletes for batch {batch_num}...")
                        for path in verified_paths:
                            if app_state["compare_cancelled"]:
                                break
                            try:
                                dbx.files_delete_v2(path)
                                deleted_count += 1
                                write_transaction("DELETE", path, "SUCCESS", "Individual delete (fallback)")
                                app_state["compare_execute_progress"]["deleted"] = deleted_count
                            except Exception as e2:
                                errors.append(f"Failed to delete {path}: {str(e2)}")
                                write_transaction("DELETE", path, "FAILED", str(e2))
                            time.sleep(0.05)  # Rate limit individual deletes
                
            else:
                # LOCAL PARALLEL DELETE with safety
                add_log(f"üóëÔ∏è Local parallel delete: {len(to_delete)} files")
                add_log(f"‚ö†Ô∏è Warning: Local deletions are PERMANENT (no trash)")
                
                def delete_local_file_safe(item):
                    """Safely delete a single local file with verification."""
                    path = item['left']['path']
                    expected_size = item['left']['size']
                    
                    try:
                        # SAFETY: Verify file exists and size matches
                        if not os.path.exists(path):
                            return ('skipped', path, "File no longer exists")
                        
                        actual_size = os.path.getsize(path)
                        if actual_size != expected_size:
                            return ('skipped', path, f"Size changed: expected {expected_size}, got {actual_size}")
                        
                        # SAFETY: Check we're not deleting a directory
                        if os.path.isdir(path):
                            return ('skipped', path, "Path is a directory, not a file")
                        
                        # Perform deletion with Safety Trash
                        success, trash_path, err = move_to_local_trash(path)
                        if success:
                            return ('success', path, f"Moved to trash: {trash_path}")
                        else:
                            return ('error', path, f"Trash failed: {err}")
                        
                    except PermissionError as e:
                        return ('error', path, f"Permission denied: {e}")
                    except OSError as e:
                        return ('error', path, f"OS error: {e}")
                    except Exception as e:
                        return ('error', path, str(e))
                
                # Use fewer threads for local ops (I/O bound)
                total_local_files = len(to_delete)
                with ThreadPoolExecutor(max_workers=4) as executor:
                    futures = {executor.submit(delete_local_file_safe, item): item for item in to_delete}
                    
                    for future in as_completed(futures):
                        if app_state["compare_cancelled"]:
                            add_log("‚ùå Cancelled by user")
                            write_transaction("SYSTEM", "N/A", "CANCELLED", "User requested cancellation")
                            executor.shutdown(wait=False, cancel_futures=True)
                            break
                        
                        status, path, error = future.result()
                        filename = os.path.basename(path)
                        
                        with app_lock:
                            if status == 'success':
                                deleted_count += 1
                                write_transaction("DELETE", path, "SUCCESS", "File removed")
                                app_state["compare_execute_progress"]["current_file"] = f"‚úÖ {filename}"
                            elif status == 'skipped':
                                skipped_count += 1
                                write_transaction("DELETE", path, "SKIPPED", error)
                            else:
                                errors.append(f"Failed: {path} - {error}")
                                write_transaction("DELETE", path, "FAILED", error)
                            
                            app_state["compare_execute_progress"]["deleted"] = deleted_count
                            app_state["compare_execute_progress"]["skipped"] = skipped_count
                            app_state["compare_execute_progress"]["current"] = deleted_count + skipped_count
                        
                        # Log progress periodically
                        total_processed = deleted_count + skipped_count
                        if total_processed % 50 == 0 or total_processed == len(to_delete):
                            add_log(f"üóëÔ∏è Progress: {deleted_count} deleted, {skipped_count} skipped")
                
                add_log(f"‚úÖ Local deletions complete: {deleted_count} deleted, {skipped_count} skipped")
        
        # =====================================================================
        # PHASE 2: COPIES (sequential for safety - overwrites are dangerous)
        # =====================================================================
        if to_copy and not app_state["compare_cancelled"]:
            total_copy = len(to_copy)
            add_log(f"üìã Starting copies: {total_copy} files")
            add_log(f"‚ö†Ô∏è Note: Copies will OVERWRITE existing files in destination")
            
            for i, item in enumerate(to_copy):
                if app_state["compare_cancelled"]:
                    add_log("‚ùå Cancelled by user")
                    write_transaction("SYSTEM", "N/A", "CANCELLED", "User requested cancellation")
                    break
                
                left_path = item['left']['path']
                right_path = item['right']['path']
                file_size = item['left']['size']
                filename = os.path.basename(left_path)
                
                # Update progress with detailed info
                app_state["compare_execute_progress"]["current"] = len(to_delete) + i + 1
                app_state["compare_execute_progress"]["current_file"] = f"üìã Copying: {filename}"
                
                try:
                    # SAFETY: Verify source file still exists before copy
                    if left_mode == "local" and not os.path.exists(left_path):
                        write_transaction("COPY", left_path, "SKIPPED", "Source file no longer exists")
                        continue
                    
                    if left_mode == "dropbox" and right_mode == "dropbox":
                        # Copy within Dropbox
                        # Check if exists and use collision handling if needed
                        # Currently we overwrite if same name, but here we can check
                        dbx.files_copy_v2(left_path, right_path, autorename=True)
                        copied_count += 1
                        with app_lock:
                            app_state["compare_execute_progress"]["bytes_current"] += file_size
                        write_transaction("COPY", f"{left_path} -> {right_path}", "SUCCESS", "Dropbox server-side copy (autorename=True)")
                        
                    elif left_mode == "local" and right_mode == "local":
                        # Copy local to local with collision check
                        dest_path = get_unique_path(right_path) if os.path.exists(right_path) else right_path
                        dest_dir = os.path.dirname(dest_path)
                        if dest_dir:
                            os.makedirs(dest_dir, exist_ok=True)
                        
                        shutil.copy2(left_path, dest_path)
                        copied_count += 1
                        with app_lock:
                            app_state["compare_execute_progress"]["bytes_current"] += file_size
                        write_transaction("COPY", f"{left_path} -> {dest_path}", "SUCCESS", "Local copy")
                        
                    elif left_mode == "local" and right_mode == "dropbox":
                        # Upload to Dropbox with autorename
                        mode = dropbox.files.WriteMode.add # This will cause autorename if conflict
                        if file_size > 5 * 1024 * 1024:  # > 5MB, use chunked
                            add_log(f"üì§ Uploading large file ({file_size // (1024*1024)}MB)...")
                            dropbox_upload_chunked(dbx, left_path, right_path)
                        else:
                            with open(left_path, 'rb') as f:
                                dbx.files_upload(f.read(), right_path, mode=dropbox.files.WriteMode.add, autorename=True)
                            with app_lock:
                                app_state["compare_execute_progress"]["bytes_current"] += file_size
                        copied_count += 1
                        write_transaction("COPY", f"{left_path} -> {right_path}", "SUCCESS", "Uploaded to Dropbox (autorename enabled)")
                        
                    elif left_mode == "dropbox" and right_mode == "local":
                        # Download from Dropbox with collision check
                        dest_path = get_unique_path(right_path) if os.path.exists(right_path) else right_path
                        dest_dir = os.path.dirname(dest_path)
                        if dest_dir:
                            os.makedirs(dest_dir, exist_ok=True)
                        dbx.files_download_to_file(dest_path, left_path)
                        copied_count += 1
                        with app_lock:
                            app_state["compare_execute_progress"]["bytes_current"] += file_size
                        write_transaction("COPY", f"{left_path} -> {dest_path}", "SUCCESS", "Downloaded from Dropbox")
                    
                    app_state["compare_execute_progress"]["copied"] = copied_count
                    
                    # Log progress periodically
                    if copied_count % 10 == 0 or copied_count == len(to_copy):
                        add_log(f"üìã Copied {copied_count}/{len(to_copy)} files")
                    
                    # SAFETY: Small delay between copies to not overwhelm I/O
                    time.sleep(0.02)
                        
                except Exception as e:
                    error_msg = f"Failed to copy {left_path}: {str(e)}"
                    errors.append(error_msg)
                    write_transaction("COPY", f"{left_path} -> {right_path}", "FAILED", str(e))
                    add_log(f"‚ö†Ô∏è {error_msg}")
            
            add_log(f"‚úÖ Copies complete: {copied_count} files")
        
        # =====================================================================
        # PHASE 3: FINALIZE AND WRITE SUMMARY
        # =====================================================================
        app_state["compare_execute_progress"]["errors"] = errors
        
        # Write summary to transaction log
        if log_filepath:
            try:
                with open(log_filepath, 'a') as f:
                    f.write(f"\n" + "=" * 80 + "\n")
                    f.write(f"EXECUTION SUMMARY\n")
                    f.write(f"=" * 80 + "\n")
                    f.write(f"Completed: {datetime.now().isoformat()}\n")
                    f.write(f"Status: {'CANCELLED' if app_state['compare_cancelled'] else 'COMPLETED'}\n")
                    f.write(f"Files deleted: {deleted_count}\n")
                    f.write(f"Files skipped: {skipped_count}\n")
                    f.write(f"Files copied: {copied_count}\n")
                    f.write(f"Errors: {len(errors)}\n")
                    if errors:
                        f.write(f"\nError Details:\n")
                        for err in errors[:20]:  # Limit to first 20 errors
                            f.write(f"  - {err}\n")
                        if len(errors) > 20:
                            f.write(f"  ... and {len(errors) - 20} more errors\n")
                    f.write(f"\nüõ°Ô∏è For Dropbox deletions: Files are in Dropbox trash for 30 days\n")
                    f.write(f"=" * 80 + "\n")
                add_log(f"üìù Transaction log saved: {log_filename}")
            except Exception as e:
                logger.warning(f"Failed to write summary to log: {e}")
        
        if not app_state["compare_cancelled"]:
            app_state["compare_execute_progress"]["status"] = "done"
            final_msg = f"üéâ Execution complete: {deleted_count} deleted"
            if skipped_count > 0:
                final_msg += f", {skipped_count} skipped"
            if copied_count > 0:
                final_msg += f", {copied_count} copied"
            if errors:
                final_msg += f", {len(errors)} errors"
            add_log(final_msg)
            add_log(f"üõ°Ô∏è Dropbox files recoverable from trash for 30 days")
            logger.info(final_msg)
        else:
            app_state["compare_execute_progress"]["status"] = "cancelled"
            add_log(f"‚ö†Ô∏è Execution was cancelled. {deleted_count} files were deleted before cancellation.")
        
    except Exception as e:
        logger.exception(f"Error during execution: {e}")
        app_state["compare_execute_progress"]["status"] = "error"
        app_state["compare_execute_progress"]["errors"].append(str(e))
        app_state["compare_execute_progress"]["log"].append(f"‚ùå Error: {str(e)}")
    
    app_state["compare_executing"] = False


# =============================================================================
# END FOLDER COMPARISON FUNCTIONS
# =============================================================================


def delete_folders():
    """
    Delete empty folders with OPTIMIZED batch processing and fail-safe verification.
    
    SAFETY FEATURES:
    - Pre-deletion verification: Each folder checked to still be empty
    - Batch processing: Up to 100 folders per API call for speed
    - Dropbox trash: Deleted folders go to trash (recoverable for 30 days)
    - Streaming log: Real-time progress updates
    - Fail-safe: Folders with new files are automatically skipped
    """
    total = len(app_state["empty_folders"])
    logger.info(f"‚ö° Starting FAST deletion of {total} empty folder(s)")
    logger.warning("‚ö†Ô∏è  DELETION OPERATION INITIATED - folders will be moved to Dropbox trash")
    logger.info("üõ°Ô∏è  FAIL-SAFE ENABLED: Each folder will be re-verified before deletion")
    
    app_state["deleting"] = True
    app_state["delete_progress"] = {
        "current": 0, 
        "total": total, 
        "status": "deleting", 
        "percent": 0,
        "deleted": 0,
        "skipped": 0,
        "errors": 0,
        "log": [
            f"üöÄ Starting fast deletion of {total} folders",
            f"üõ°Ô∏è Safety checks enabled - folders will be verified",
            f"üóëÔ∏è Deleted folders go to Dropbox trash (30-day recovery)"
        ]
    }
    
    def add_log(msg):
        """Add to streaming log."""
        app_state["delete_progress"]["log"].append(msg)
        logger.info(msg)
    
    dbx = app_state["dbx"]
    deleted_count = 0
    skipped_count = 0
    error_count = 0
    start_time = time.time()
    
    # Sort folders by depth (deepest first) to avoid parent-before-child issues
    folders_to_delete = sorted(app_state["empty_folders"], key=lambda x: x.count('/'), reverse=True)
    
    # PHASE 1: PARALLEL verification of all folders
    add_log(f"üìã Phase 1: ‚ö° Parallel verification of {total} folders...")
    verified_folders = []
    verification_lock = threading.Lock()
    
    def verify_single_folder(folder):
        """Verify a single folder in parallel."""
        display_path = app_state["case_map"].get(folder, folder)
        is_empty, file_count, error = verify_folder_empty(dbx, folder)
        return (folder, display_path, is_empty, file_count, error)
    
    # Use ThreadPoolExecutor for parallel verification (4 threads to balance API load)
    PARALLEL_WORKERS = 4
    verified_count = 0
    
    with ThreadPoolExecutor(max_workers=PARALLEL_WORKERS) as executor:
        futures = {executor.submit(verify_single_folder, f): f for f in folders_to_delete}
        
        for future in as_completed(futures):
            folder, display_path, is_empty, file_count, error = future.result()
            
            with verification_lock:
                verified_count += 1
                app_state["delete_progress"]["current"] = verified_count
                app_state["delete_progress"]["percent"] = int((verified_count / total) * 50) if total > 0 else 0
                
                if error == "folder_not_found":
                    deleted_count += 1
                    # Only log every 10th or if few folders
                    if total < 20 or verified_count % 10 == 0:
                        add_log(f"‚äò Already gone: {display_path}")
                elif error:
                    error_count += 1
                    add_log(f"‚ö†Ô∏è Verification failed: {display_path}")
                elif not is_empty:
                    skipped_count += 1
                    add_log(f"üõ°Ô∏è FAIL-SAFE: {display_path} has {file_count} file(s) - SKIPPED")
                else:
                    verified_folders.append(folder)
            
            # Log progress periodically
            if verified_count % 50 == 0 or verified_count == total:
                add_log(f"‚úÖ Verified {verified_count}/{total}: {len(verified_folders)} ready, {skipped_count} skipped")
    
    app_state["delete_progress"]["skipped"] = skipped_count
    app_state["delete_progress"]["errors"] = error_count
    
    verify_time = time.time() - start_time
    add_log(f"‚ö° Verification complete in {verify_time:.1f}s: {len(verified_folders)} ready to delete")
    
    if not verified_folders:
        add_log(f"üìã No folders to delete (all already gone, skipped, or errored)")
        app_state["delete_progress"]["status"] = "complete"
        app_state["delete_progress"]["percent"] = 100
        app_state["empty_folders"] = []
        app_state["deleting"] = False
        return
    
    # PHASE 2: Batch delete verified folders
    add_log(f"üóëÔ∏è Phase 2: ‚ö° Batch deleting {len(verified_folders)} verified folders...")
    
    DELETE_BATCH_SIZE = 100  # Dropbox allows up to 1000, but smaller is safer
    
    for batch_start in range(0, len(verified_folders), DELETE_BATCH_SIZE):
        batch = verified_folders[batch_start:batch_start + DELETE_BATCH_SIZE]
        batch_num = (batch_start // DELETE_BATCH_SIZE) + 1
        total_batches = (len(verified_folders) + DELETE_BATCH_SIZE - 1) // DELETE_BATCH_SIZE
        
        progress_base = 50 + int((batch_start / len(verified_folders)) * 50)
        app_state["delete_progress"]["percent"] = progress_base
        app_state["delete_progress"]["current"] = deleted_count + skipped_count + error_count
        
        add_log(f"üì¶ Batch {batch_num}/{total_batches}: Deleting {len(batch)} folders...")
        
        try:
            # Use Dropbox batch delete API
            entries = [dropbox.files.DeleteArg(path) for path in batch]
            result = dbx.files_delete_batch(entries)
            
            # Handle async job
            if result.is_async_job_id():
                job_id = result.get_async_job_id()
                add_log(f"‚è≥ Batch {batch_num} processing (async)...")
                
                poll_count = 0
                max_polls = 120  # 60 second timeout
                while poll_count < max_polls:
                    time.sleep(0.5)
                    poll_count += 1
                    check = dbx.files_delete_batch_check(job_id)
                    if check.is_complete():
                        batch_result = check.get_complete()
                        break
                    elif check.is_failed():
                        add_log(f"‚ö†Ô∏è Batch {batch_num} failed")
                        batch_result = None
                        break
                else:
                    add_log(f"‚ö†Ô∏è Batch {batch_num} timed out")
                    batch_result = None
            else:
                batch_result = result.get_complete()
            
            # Count results
            if batch_result:
                batch_deleted = 0
                batch_errors = 0
                for entry in batch_result.entries:
                    if entry.is_success():
                        deleted_count += 1
                        batch_deleted += 1
                    elif entry.is_failure():
                        error_count += 1
                        batch_errors += 1
                
                add_log(f"‚úÖ Batch {batch_num}: {batch_deleted} deleted" + 
                       (f", {batch_errors} errors" if batch_errors > 0 else ""))
            else:
                # Batch failed - fallback to individual deletes
                add_log(f"üîÑ Falling back to individual deletes for batch {batch_num}...")
                for folder in batch:
                    try:
                        dbx.files_delete_v2(folder)
                        deleted_count += 1
                    except ApiError as e:
                        if 'not_found' in str(e).lower():
                            deleted_count += 1
                        else:
                            error_count += 1
                    except:
                        error_count += 1
                    time.sleep(0.02)  # Rate limit
            
            app_state["delete_progress"]["deleted"] = deleted_count
            app_state["delete_progress"]["errors"] = error_count
            
            # Rate limit between batches
            time.sleep(0.2)
            
        except Exception as e:
            add_log(f"‚ö†Ô∏è Batch {batch_num} error: {str(e)}")
            # Fallback to individual deletes
            for folder in batch:
                try:
                    dbx.files_delete_v2(folder)
                    deleted_count += 1
                except:
                    error_count += 1
                time.sleep(0.02)
    
    elapsed = time.time() - start_time
    app_state["empty_folders"] = []
    app_state["delete_progress"]["status"] = "complete"
    app_state["delete_progress"]["percent"] = 100
    app_state["delete_progress"]["deleted"] = deleted_count
    app_state["delete_progress"]["skipped"] = skipped_count
    app_state["delete_progress"]["errors"] = error_count
    app_state["deleting"] = False
    
    # Final summary
    final_msg = f"üéâ Deletion complete: {deleted_count} deleted"
    if skipped_count > 0:
        final_msg += f", {skipped_count} skipped (safety)"
    if error_count > 0:
        final_msg += f", {error_count} errors"
    final_msg += f" in {elapsed:.1f}s"
    add_log(final_msg)
    add_log(f"üõ°Ô∏è Deleted folders are in Dropbox trash for 30 days")
    
    # Detailed completion log
    logger.info(f"=" * 60)
    logger.info(f"DELETION COMPLETE")
    logger.info(f"=" * 60)
    logger.info(f"  ‚úì Successfully deleted: {deleted_count}")
    logger.info(f"  üõ°Ô∏è  Skipped (fail-safe): {skipped_count}")
    logger.info(f"  ‚úó Errors: {error_count}")
    logger.info(f"  ‚è±Ô∏è  Time: {elapsed:.2f}s")
    logger.info(f"=" * 60)


def delete_conflict_files():
    """Delete identified conflict files."""
    mode = app_state["config"].get("mode", "dropbox")
    conflicts = app_state.get("conflicts", [])
    
    if not conflicts:
        logger.info("No conflict files to delete")
        return
        
    start_time = time.time()
    app_state["deleting"] = True
    app_state["delete_progress"] = {
        "current": 0, 
        "total": len(conflicts), 
        "status": "deleting", 
        "percent": 0,
        "deleted": 0,
        "skipped": 0,
        "errors": 0,
        "log": []
    }
    
    logger.info(f"Starting deletion of {len(conflicts)} conflict files in {mode} mode")
    
    try:
        if mode == "dropbox":
            dbx = app_state["dbx"]
            if not dbx:
                app_state["delete_progress"]["status"] = "error"
                app_state["delete_progress"]["log"].append("Error: Not connected to Dropbox")
                app_state["deleting"] = False
                return

            for i, item in enumerate(conflicts):
                # check cancellation
                if app_state["scan_cancelled"] or app_state["compare_cancelled"]: # Reusing cancel flags
                     logger.info("Conflict deletion cancelled")
                     app_state["delete_progress"]["status"] = "cancelled"
                     break

                path = item["path"]
                try:
                    dbx.files_delete_v2(path)
                    app_state["delete_progress"]["deleted"] += 1
                    app_state["delete_progress"]["log"].append(f"Deleted: {path}")
                except Exception as e:
                    app_state["delete_progress"]["errors"] += 1
                    app_state["delete_progress"]["log"].append(f"Error deleting {path}: {str(e)}")
                
                app_state["delete_progress"]["current"] = i + 1
                app_state["delete_progress"]["percent"] = int(((i + 1) / len(conflicts)) * 100)
                
        else: # Local
            # Verify we have move_to_local_trash available
            config = app_state["config"]
            base_path = config.get("local_path", "")
            
            for i, item in enumerate(conflicts):
                # check cancellation
                if app_state["scan_cancelled"] or app_state["compare_cancelled"]:
                     logger.info("Conflict deletion cancelled")
                     app_state["delete_progress"]["status"] = "cancelled"
                     break
                     
                path = item["path"]
                # path from conflicts stored is either full absolute path (local) or relative?
                # In scan_local_folder: `all_files.append(file_path)` where `file_path = norm_path + '/' + filename` (relative to base)
                # So we need to join with base_path.
                
                full_path = os.path.join(base_path, path.lstrip('/'))
                
                try:
                    success, trash_path, error = move_to_local_trash(full_path)
                    if success:
                        app_state["delete_progress"]["deleted"] += 1
                        app_state["delete_progress"]["log"].append(f"Moved to trash: {path}")
                    else:
                        app_state["delete_progress"]["errors"] += 1
                        app_state["delete_progress"]["log"].append(f"Error: {error} for {path}")
                except Exception as e:
                    app_state["delete_progress"]["errors"] += 1
                    app_state["delete_progress"]["log"].append(f"Exception deleting {path}: {str(e)}")
                    
                app_state["delete_progress"]["current"] = i + 1
                app_state["delete_progress"]["percent"] = int(((i + 1) / len(conflicts)) * 100)
                
    except Exception as e:
        logger.error(f"Error in delete_conflict_files: {e}")
        app_state["delete_progress"]["status"] = "error"
        app_state["delete_progress"]["log"].append(f"Critical error: {str(e)}")
        
    app_state["deleting"] = False
    if app_state["delete_progress"]["status"] != "cancelled" and app_state["delete_progress"]["status"] != "error":
        app_state["delete_progress"]["status"] = "complete"
    
    # Refresh conflicts list (clear deleted ones) - optional, but let's just clear for now if successful
    if app_state["delete_progress"]["status"] == "complete":
        app_state["conflicts"] = []


def save_config():
    """Save current configuration to config.json."""
    try:
        config_path = os.path.join(os.path.dirname(__file__), CONFIG_FILE)
        with open(config_path, 'w') as f:
            json.dump(app_state["config"], f, indent=4)
        logger.info("Configuration saved")
    except Exception as e:
        logger.error(f"Failed to save config: {e}")

def load_config():
    """Load configuration from config.json."""
    try:
        config_path = os.path.join(os.path.dirname(__file__), CONFIG_FILE)
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                saved_config = json.load(f)
                # update app_state config, preserving defaults for missing keys
                app_state["config"].update(saved_config)
            logger.info("Configuration loaded")
        else:
            logger.info("No config file found, using defaults")
            save_config() # Create default file
    except Exception as e:
        logger.error(f"Failed to load config: {e}")

# Load config on module import
load_config()


